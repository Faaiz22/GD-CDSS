"""
Generate all required artifacts from Colab notebook outputs.
Run this ONCE after uploading Colab-generated files to data/raw/
"""

import sys
import os
import json
import numpy as np
import pandas as pd
import torch
from pathlib import Path
from tqdm import tqdm

# Add project root to path
ROOT = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "src"))

from src.features.drug_featurizer import DrugFeaturizer
from src.features.protein_featurizer import ProteinFeaturizer

print("=" * 70)
print("COLAB ARTIFACT GENERATOR")
print("=" * 70)

# Create directories
ARTIFACTS_DIR = ROOT / "artifacts"
DATA_RAW = ROOT / "data" / "raw"
ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)
DATA_RAW.mkdir(exist_ok=True, parents=True)

print(f"\nüìÅ Artifacts directory: {ARTIFACTS_DIR}")
print(f"üìÅ Raw data directory: {DATA_RAW}")

# ============================================================================
# STEP 1: Check for Colab-generated CSV files
# ============================================================================
print("\n" + "=" * 70)
print("STEP 1: Checking for Colab-generated files")
print("=" * 70)

REQUIRED_COLAB_FILES = {
    "unified_data": "Unified_Gene-Drug_Association_Protein_Features.csv",
    "phyto_candidates": None,  # Optional: generated candidate file
}

# Check for main unified dataset
unified_csv_path = None
for candidate in [
    DATA_RAW / "Unified_Gene-Drug_Association_Protein_Features.csv",
    DATA_RAW / "Unified_Gene-Drug_Association_3D_Features.csv",
    DATA_RAW / "Unified_Gene-Drug_Association_with_Sequences.csv",
]:
    if candidate.exists():
        unified_csv_path = candidate
        print(f"‚úÖ Found unified dataset: {candidate.name}")
        break

if unified_csv_path is None:
    print("\n‚ùå ERROR: No unified CSV file found!")
    print("\nRequired files (place ANY ONE in data/raw/):")
    print("  - Unified_Gene-Drug_Association_Protein_Features.csv (preferred)")
    print("  - Unified_Gene-Drug_Association_3D_Features.csv")
    print("  - Unified_Gene-Drug_Association_with_Sequences.csv")
    print("\nThese are generated by your Colab notebook.")
    sys.exit(1)

# Load unified data
print(f"\nüìÇ Loading {unified_csv_path.name}...")
df = pd.read_csv(unified_csv_path, low_memory=False)
print(f"   Shape: {df.shape}")
print(f"   Columns: {len(df.columns)}")

# ============================================================================
# STEP 2: Extract Gene Information
# ============================================================================
print("\n" + "=" * 70)
print("STEP 2: Extracting Gene Information")
print("=" * 70)

# Find gene ID column (auto-detect)
gene_id_col = None
for col in df.columns:
    if "gene" in col.lower() and "id" in col.lower():
        gene_id_col = col
        break

if gene_id_col is None:
    print("‚ùå Could not find Gene ID column")
    sys.exit(1)

print(f"‚úÖ Detected Gene ID column: {gene_id_col}")

# Find gene symbol column
gene_symbol_col = None
for col in df.columns:
    if "gene" in col.lower() and ("name" in col.lower() or "symbol" in col.lower()):
        gene_symbol_col = col
        break

if gene_symbol_col is None:
    # Try to extract from Gene_ID if format is like "HGNC:1234"
    print("‚ö†Ô∏è  No gene symbol column found, will use Gene_ID")
    gene_symbol_col = gene_id_col

print(f"‚úÖ Detected Gene Symbol column: {gene_symbol_col}")

# Get unique genes
unique_genes = df[[gene_id_col, gene_symbol_col]].drop_duplicates()
unique_genes = unique_genes.dropna()
unique_genes = unique_genes.sort_values(gene_id_col)

ordered_gene_ids = unique_genes[gene_id_col].astype(str).tolist()
ordered_gene_symbols = unique_genes[gene_symbol_col].astype(str).tolist()

print(f"‚úÖ Found {len(ordered_gene_ids)} unique genes")
print(f"   Sample genes: {ordered_gene_symbols[:5]}")

# ============================================================================
# STEP 3: Extract Drug Information
# ============================================================================
print("\n" + "=" * 70)
print("STEP 3: Extracting Drug Information")
print("=" * 70)

# Find drug ID column
drug_id_col = None
for col in df.columns:
    if "drug" in col.lower() and "id" in col.lower():
        drug_id_col = col
        break

if drug_id_col is None:
    print("‚ùå Could not find Drug ID column")
    sys.exit(1)

print(f"‚úÖ Detected Drug ID column: {drug_id_col}")

# Find SMILES column
smiles_col = None
for col in df.columns:
    if "smiles" in col.lower():
        smiles_col = col
        break

if smiles_col is None:
    print("‚ùå Could not find SMILES column")
    sys.exit(1)

print(f"‚úÖ Detected SMILES column: {smiles_col}")

# Get unique drugs
unique_drugs = df[[drug_id_col, smiles_col]].drop_duplicates()
unique_drugs = unique_drugs.dropna(subset=[smiles_col])
unique_drugs = unique_drugs.sort_values(drug_id_col)

ordered_drug_ids = unique_drugs[drug_id_col].astype(str).tolist()
ordered_drug_smiles = unique_drugs[smiles_col].astype(str).tolist()

print(f"‚úÖ Found {len(ordered_drug_ids)} unique drugs")
print(f"   Sample SMILES: {ordered_drug_smiles[0][:50]}...")

# ============================================================================
# STEP 4: Build Drug Feature Vectors (135-dim)
# ============================================================================
print("\n" + "=" * 70)
print("STEP 4: Building Drug Feature Vectors (135-dim)")
print("=" * 70)

print("üîß Initializing Drug Featurizer...")
drug_feat = DrugFeaturizer()

print("   Fitting descriptor scaler...")
drug_feat.fit_descriptor_scaler(ordered_drug_smiles)

print("   Fitting fingerprint PCA (128 components)...")
drug_feat.fit_fp_pca(ordered_drug_smiles, n_components=128)

print("   Generating feature vectors...")
drug_vectors = []
failed_drugs = []

for i, smi in enumerate(tqdm(ordered_drug_smiles, desc="Drug vectors")):
    try:
        vec = drug_feat.build_feature_vector(smi)
        drug_vectors.append(vec)
    except Exception as e:
        # Use zero vector for failed drugs
        drug_vectors.append(np.zeros(135))
        failed_drugs.append((i, smi[:30], str(e)))

drug_vectors = np.vstack(drug_vectors)

print(f"‚úÖ Generated {len(drug_vectors)} drug vectors (shape: {drug_vectors.shape})")
if failed_drugs:
    print(f"‚ö†Ô∏è  {len(failed_drugs)} drugs failed (using zero vectors)")

# Save drug featurizer
drug_feat.save(str(ARTIFACTS_DIR / "drug_featurizer"))
print(f"‚úÖ Saved drug featurizer to: {ARTIFACTS_DIR / 'drug_featurizer_*.pkl'}")

# Save drug library
np.save(ARTIFACTS_DIR / "drug_library.npy", drug_vectors)
print(f"‚úÖ Saved drug library: {ARTIFACTS_DIR / 'drug_library.npy'}")

# ============================================================================
# STEP 5: Build Protein Feature Vectors (38-dim)
# ============================================================================
print("\n" + "=" * 70)
print("STEP 5: Building Protein Feature Vectors (38-dim)")
print("=" * 70)

# Find protein sequence column
prot_seq_col = None
for col in df.columns:
    if "protein" in col.lower() and "seq" in col.lower():
        prot_seq_col = col
        break

if prot_seq_col is None:
    print("‚ö†Ô∏è  No Protein_Sequence column found!")
    print("   Will use dummy sequences (not recommended)")
    protein_sequences = [""] * len(ordered_gene_ids)
else:
    print(f"‚úÖ Detected Protein Sequence column: {prot_seq_col}")
    
    # Map gene IDs to sequences
    gene_to_seq = {}
    for _, row in df.iterrows():
        gid = str(row.get(gene_id_col, ""))
        seq = str(row.get(prot_seq_col, ""))
        if gid and seq and seq != "nan":
            gene_to_seq[gid] = seq
    
    protein_sequences = [gene_to_seq.get(gid, "") for gid in ordered_gene_ids]
    
    non_empty = sum(1 for s in protein_sequences if s)
    print(f"   Found {non_empty}/{len(protein_sequences)} protein sequences")

print("\nüîß Initializing Protein Featurizer...")
prot_feat = ProteinFeaturizer()

print("   Fitting DPC PCA (31 components)...")
prot_feat.fit_dpc_pca(protein_sequences, n_components=31)

print("   Fitting physicochemical scaler...")
prot_feat.fit_physchem_scaler(protein_sequences)

print("   Generating feature vectors...")
protein_vectors = []

for seq in tqdm(protein_sequences, desc="Protein vectors"):
    try:
        vec = prot_feat.build_feature_vector(seq)
        protein_vectors.append(vec)
    except Exception as e:
        protein_vectors.append(np.zeros(38))

protein_vectors = np.vstack(protein_vectors)

print(f"‚úÖ Generated {len(protein_vectors)} protein vectors (shape: {protein_vectors.shape})")

# Save protein featurizer
prot_feat.save(str(ARTIFACTS_DIR / "protein_featurizer"))
print(f"‚úÖ Saved protein featurizer to: {ARTIFACTS_DIR / 'protein_featurizer_*.pkl'}")

# Save protein library
np.save(ARTIFACTS_DIR / "protein_library.npy", protein_vectors)
print(f"‚úÖ Saved protein library: {ARTIFACTS_DIR / 'protein_library.npy'}")

# ============================================================================
# STEP 6: Build Association Dataset
# ============================================================================
print("\n" + "=" * 70)
print("STEP 6: Building Association Dataset")
print("=" * 70)

# Find association label column
label_col = None
for col in df.columns:
    if "association" in col.lower() and "label" in col.lower():
        label_col = col
        break

if label_col is None:
    print("‚ö†Ô∏è  No Association_Label column found, assuming all positive")
    df["Association_Label"] = 1

# Build ID mappings
gene_id_to_index = {str(gid): i for i, gid in enumerate(ordered_gene_ids)}
drug_id_to_index = {str(did): i for i, did in enumerate(ordered_drug_ids)}

X_drug, X_prot, Y = [], [], []
positive_pairs = []

print("üîß Processing gene-drug pairs...")
for _, row in tqdm(df.iterrows(), total=len(df), desc="Association pairs"):
    gid = str(row.get(gene_id_col, ""))
    did = str(row.get(drug_id_col, ""))
    
    if gid not in gene_id_to_index or did not in drug_id_to_index:
        continue
    
    gidx = gene_id_to_index[gid]
    didx = drug_id_to_index[did]
    
    dv = drug_vectors[didx]
    pv = protein_vectors[gidx]
    
    label_val = row.get(label_col, 1)
    label = 1.0 if label_val == 1 or str(label_val).lower() in ["associated", "true", "1"] else 0.0
    
    X_drug.append(dv)
    X_prot.append(pv)
    Y.append(label)
    
    if label == 1.0:
        positive_pairs.append(dv)

X_drug = torch.tensor(np.vstack(X_drug), dtype=torch.float32)
X_prot = torch.tensor(np.vstack(X_prot), dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.float32).unsqueeze(1)

print(f"‚úÖ Built association dataset:")
print(f"   Total pairs: {len(Y)}")
print(f"   Positive: {int(Y.sum())}")
print(f"   Negative: {len(Y) - int(Y.sum())}")

# Save association dataset
torch.save(
    {"drug": X_drug, "prot": X_prot, "label": Y},
    ARTIFACTS_DIR / "association_dataset.pt"
)
print(f"‚úÖ Saved: {ARTIFACTS_DIR / 'association_dataset.pt'}")

# Save positive pairs for C-VAE
if positive_pairs:
    positive_tensor = torch.tensor(np.vstack(positive_pairs), dtype=torch.float32)
    torch.save(positive_tensor, ARTIFACTS_DIR / "positive_feature_pairs.pt")
    print(f"‚úÖ Saved: {ARTIFACTS_DIR / 'positive_feature_pairs.pt'} ({len(positive_pairs)} pairs)")

# ============================================================================
# STEP 7: Process Phytochemicals (if available)
# ============================================================================
print("\n" + "=" * 70)
print("STEP 7: Processing Phytochemicals (Optional)")
print("=" * 70)

phyto_csv = DATA_RAW / "phytochemicals_new1.csv"
if phyto_csv.exists():
    print(f"‚úÖ Found phytochemical file: {phyto_csv.name}")
    
    try:
        phyto_df = pd.read_csv(phyto_csv, dtype=str)
        
        # Auto-detect SMILES column
        phyto_smiles_col = None
        for col in phyto_df.columns:
            if "smiles" in col.lower():
                phyto_smiles_col = col
                break
        
        if phyto_smiles_col is None:
            print("   ‚ö†Ô∏è  No SMILES column in phytochemicals")
        else:
            phyto_df = phyto_df.dropna(subset=[phyto_smiles_col])
            phyto_smiles_list = phyto_df[phyto_smiles_col].astype(str).tolist()
            
            print(f"   Found {len(phyto_smiles_list)} phytochemicals")
            print("   Generating feature vectors...")
            
            phyto_vectors = []
            for smi in tqdm(phyto_smiles_list, desc="Phyto vectors"):
                try:
                    vec = drug_feat.build_feature_vector(smi)
                    phyto_vectors.append(vec)
                except:
                    phyto_vectors.append(np.zeros(135))
            
            phyto_vectors = np.vstack(phyto_vectors)
            
            # Save
            np.save(ARTIFACTS_DIR / "phyto_library.npy", phyto_vectors)
            phyto_df.to_parquet(ARTIFACTS_DIR / "phyto_metadata.parquet", index=False)
            
            print(f"   ‚úÖ Saved phyto_library.npy ({phyto_vectors.shape})")
            print(f"   ‚úÖ Saved phyto_metadata.parquet")
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Failed to process phytochemicals: {e}")
else:
    print("‚ö†Ô∏è  No phytochemical file found (optional)")
    print("   Skipping phyto_library.npy and phyto_metadata.parquet")

# ============================================================================
# STEP 8: Create ID Maps
# ============================================================================
print("\n" + "=" * 70)
print("STEP 8: Creating ID Maps")
print("=" * 70)

gene_id_to_symbol = dict(zip(ordered_gene_ids, ordered_gene_symbols))

id_maps = {
    "ordered_gene_ids": ordered_gene_ids,
    "ordered_symbols_list": ordered_gene_symbols,
    "ordered_drug_ids": ordered_drug_ids,
    "gene_id_to_index": gene_id_to_index,
    "drug_id_to_index": drug_id_to_index,
    "gene_id_to_symbol": gene_id_to_symbol,
}

with open(ARTIFACTS_DIR / "id_maps.json", "w") as f:
    json.dump(id_maps, f, indent=2)

print(f"‚úÖ Saved: {ARTIFACTS_DIR / 'id_maps.json'}")

# ============================================================================
# STEP 9: Create Feature Metadata
# ============================================================================
print("\n" + "=" * 70)
print("STEP 9: Creating Feature Metadata")
print("=" * 70)

metadata = {
    "drug_vector_dim": 135,
    "protein_vector_dim": 38,
    "n_genes": len(ordered_gene_ids),
    "n_drugs": len(ordered_drug_ids),
    "n_associations": len(Y),
    "n_positive": int(Y.sum()),
    "source": "colab_notebook",
}

with open(ARTIFACTS_DIR / "feature_metadata.json", "w") as f:
    json.dump(metadata, f, indent=2)

print(f"‚úÖ Saved: {ARTIFACTS_DIR / 'feature_metadata.json'}")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "=" * 70)
print("ARTIFACT GENERATION COMPLETE!")
print("=" * 70)

print("\nüì¶ Generated Artifacts:")
artifacts = [
    "drug_featurizer_desc_scaler.pkl",
    "drug_featurizer_fp_scaler.pkl",
    "drug_featurizer_fp_pca.pkl",
    "protein_featurizer_physchem_scaler.pkl",
    "protein_featurizer_dpc_scaler.pkl",
    "protein_featurizer_dpc_pca.pkl",
    "drug_library.npy",
    "protein_library.npy",
    "association_dataset.pt",
    "positive_feature_pairs.pt",
    "id_maps.json",
    "feature_metadata.json",
]

optional_artifacts = [
    "phyto_library.npy",
    "phyto_metadata.parquet",
]

for art in artifacts:
    path = ARTIFACTS_DIR / art
    if path.exists():
        size_mb = path.stat().st_size / (1024 * 1024)
        print(f"   ‚úÖ {art} ({size_mb:.2f} MB)")
    else:
        print(f"   ‚ùå {art} (MISSING)")

print("\nüì¶ Optional Artifacts:")
for art in optional_artifacts:
    path = ARTIFACTS_DIR / art
    if path.exists():
        size_mb = path.stat().st_size / (1024 * 1024)
        print(f"   ‚úÖ {art} ({size_mb:.2f} MB)")
    else:
        print(f"   ‚ö†Ô∏è  {art} (not generated)")

print("\nüéØ Next Steps:")
print("   1. Train association model: python scripts/02_train_association_model.py")
print("   2. Train C-VAE model: python scripts/03_train_cvae.py")
print("   3. Run Streamlit app: streamlit run streamlit_app.py")

print("\n‚ú® All done!")
